import torch
import gc
import numpy as np
from PIL import Image
from transformers import AutoProcessor, AutoModelForPreTraining
from transformers import LlavaOnevisionForConditionalGeneration
import os


class TestRoboBrain:
    """
    RoboBrainæ¨¡å‹æµ‹è¯•ç±»
    åŸºäºBAAI/RoboBrainæ¨¡å‹ï¼Œæ”¯æŒå›¾åƒå’Œæ–‡æœ¬çš„å¤šæ¨¡æ€æ¨ç†
    """
    
    def __init__(self, device=None, multi_gpu=None):
        """
        åˆå§‹åŒ–RoboBrainæ¨¡å‹
        
        Args:
            device: è®¡ç®—è®¾å¤‡ï¼Œé»˜è®¤ä¸ºcuda:0
            multi_gpu: æ˜¯å¦ä½¿ç”¨å¤šå¡ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
        """
        # æ£€æµ‹GPUè®¾å¤‡
        self.gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
        
        if device is None:
            self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        else:
            self.device = device
        
        # å¤šå¡è®¾ç½®
        if multi_gpu is None:
            self.use_multi_gpu = self.gpu_count > 1
        else:
            self.use_multi_gpu = multi_gpu and self.gpu_count > 1
            
        print(f"æ­£åœ¨åˆå§‹åŒ–RoboBrainæ¨¡å‹ï¼Œè®¾å¤‡: {self.device}")
        print(f"æ£€æµ‹åˆ° {self.gpu_count} ä¸ªGPUè®¾å¤‡")
        if self.use_multi_gpu:
            print(f"âœ… å¯ç”¨å¤šå¡æ¨ç†ï¼Œä½¿ç”¨ {self.gpu_count} ä¸ªGPU")
        else:
            print(f"ä½¿ç”¨å•å¡æ¨ç†")
        
        # æ¨¡å‹è·¯å¾„ - ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹
        local_model_path = "/home/fx/Exp2/test/EmbodiedEval/msjeval/model_zoo/RoboBrain"
        if os.path.exists(local_model_path):
            self.model_path = local_model_path
            print(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {self.model_path}")
        else:
            self.model_path = "BAAI/RoboBrain"
            print(f"ä½¿ç”¨HuggingFaceæ¨¡å‹: {self.model_path}")
        
        self.model = None
        self.processor = None
        self.is_model_parallel = False
        self.load_model()
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
        try:
            print("æ­£åœ¨åŠ è½½RoboBrainæ¨¡å‹...")
            
            # åŠ è½½å¤„ç†å™¨
            self.processor = AutoProcessor.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            print("âœ… å¤„ç†å™¨åŠ è½½æˆåŠŸ")
            
            # åŠ è½½æ¨¡å‹ - æ ¹æ®å¤šå¡è®¾ç½®é€‰æ‹©ä¸åŒçš„åŠ è½½æ–¹å¼
            if self.use_multi_gpu and self.gpu_count > 1:
                print(f"ğŸ”„ ä½¿ç”¨å¤šå¡åŠ è½½æ¨¡å‹åˆ° {self.gpu_count} ä¸ªGPU...")
                # ä½¿ç”¨device_map="auto"è®©transformersè‡ªåŠ¨åˆ†é…æ¨¡å‹åˆ°å¤šä¸ªGPU
                self.model = LlavaOnevisionForConditionalGeneration.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float16,
                    low_cpu_mem_usage=True,
                    trust_remote_code=True,
                    device_map="auto"  # è‡ªåŠ¨åˆ†é…åˆ°å¤šä¸ªGPU
                )
                self.is_model_parallel = True
                print(f"âœ… æ¨¡å‹å·²åˆ†å¸ƒåˆ°å¤šä¸ªGPU")
                
                # æ‰“å°è®¾å¤‡åˆ†é…ä¿¡æ¯
                if hasattr(self.model, 'hf_device_map'):
                    print("ğŸ“Š è®¾å¤‡åˆ†é…æ˜ å°„:")
                    for layer, device in self.model.hf_device_map.items():
                        print(f"  {layer}: {device}")
                
            else:
                print(f"ğŸ”„ ä½¿ç”¨å•å¡åŠ è½½æ¨¡å‹åˆ° {self.device}...")
                self.model = LlavaOnevisionForConditionalGeneration.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float16,
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
                
                # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                if torch.cuda.is_available():
                    self.model = self.model.to(self.device)
                
                self.is_model_parallel = False
                print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° {self.device}")
                
            print("âœ… RoboBrainæ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise e
    
    def preprocess_image(self, image):
        """
        é¢„å¤„ç†å›¾åƒ
        
        Args:
            image: å›¾åƒï¼Œå¯ä»¥æ˜¯PIL Imageã€numpy arrayã€listæˆ–è·¯å¾„å­—ç¬¦ä¸²
            
        Returns:
            PIL Imageå¯¹è±¡
        """
        if isinstance(image, str):
            return Image.open(image).convert("RGB")
        elif isinstance(image, np.ndarray):
            return Image.fromarray(image.astype('uint8')).convert('RGB')
        elif isinstance(image, Image.Image):
            return image.convert("RGB")
        elif isinstance(image, list):
            # å¦‚æœæ˜¯listï¼Œè½¬æ¢ä¸ºnumpy arrayç„¶åå¤„ç†
            image_array = np.array(image, dtype='uint8')
            return Image.fromarray(image_array).convert('RGB')
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒç±»å‹: {type(image)}")
    
    def generate_image(self, image_list_list, question_list, max_new_tokens=512, temperature=0.7, do_sample=True):
        """
        åŸºäºå›¾åƒç”Ÿæˆå›ç­”
        
        Args:
            image_list_list: å›¾åƒåˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå­åˆ—è¡¨åŒ…å«ä¸€ä¸ªæ ·æœ¬çš„æ‰€æœ‰å›¾åƒ
            question_list: é—®é¢˜åˆ—è¡¨
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: é‡‡æ ·æ¸©åº¦
            do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·
            
        Returns:
            ç”Ÿæˆçš„å›ç­”åˆ—è¡¨
        """
        if len(image_list_list) != len(question_list):
            raise ValueError("å›¾åƒåˆ—è¡¨å’Œé—®é¢˜åˆ—è¡¨é•¿åº¦ä¸åŒ¹é…")
        
        results = []
        
        for images, question in zip(image_list_list, question_list):
            try:
                # å¤„ç†å›¾åƒ
                if len(images) == 0:
                    print("è­¦å‘Š: ç©ºå›¾åƒåˆ—è¡¨ï¼Œè·³è¿‡æ­¤æ ·æœ¬")
                    results.append("æ— å›¾åƒè¾“å…¥")
                    continue
                
                # å–ç¬¬ä¸€å¼ å›¾åƒï¼ˆRoboBrainä¸»è¦å¤„ç†å•å›¾åƒï¼‰
                image = self.preprocess_image(images[0])
                
                # æ„å»ºæ¶ˆæ¯æ ¼å¼
                messages = [
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": question},
                            {"type": "image", "image": image},
                        ],
                    },
                ]
                
                # å¤„ç†è¾“å…¥
                inputs = self.processor.apply_chat_template(
                    messages,
                    add_generation_prompt=True,
                    tokenize=True,
                    return_dict=True,
                    return_tensors="pt"
                )
                
                # ç§»åŠ¨åˆ°è®¾å¤‡ - å¤šå¡æƒ…å†µä¸‹ä¸éœ€è¦æ‰‹åŠ¨ç§»åŠ¨
                if not self.is_model_parallel:
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                else:
                    # å¤šå¡æƒ…å†µä¸‹ï¼Œè¾“å…¥æ•°æ®ç§»åŠ¨åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨è®¾å¤‡
                    first_device = next(iter(self.model.hf_device_map.values()))
                    inputs = {k: v.to(first_device) for k, v in inputs.items()}
                
                # ç”Ÿæˆå›ç­”
                with torch.no_grad():
                    output = self.model.generate(
                        **inputs,
                        max_new_tokens=max_new_tokens,
                        do_sample=do_sample,
                        temperature=temperature,
                        pad_token_id=self.processor.tokenizer.eos_token_id
                    )
                
                # è§£ç è¾“å‡º
                prediction = self.processor.decode(
                    output[0][2:],
                    skip_special_tokens=True
                ).split("assistant")[-1].strip()
                
                results.append(prediction)
                
            except Exception as e:
                print(f"å¤„ç†æ ·æœ¬æ—¶å‡ºé”™: {e}")
                results.append(f"å¤„ç†é”™è¯¯: {str(e)}")
                
        return results
    
    def generate_clip(self, video_list_list, question_list, max_new_tokens=512, temperature=0.7, do_sample=True):
        """
        åŸºäºè§†é¢‘/å›¾åƒåºåˆ—ç”Ÿæˆå›ç­”
        
        Args:
            video_list_list: è§†é¢‘å¸§åˆ—è¡¨çš„åˆ—è¡¨
            question_list: é—®é¢˜åˆ—è¡¨
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: é‡‡æ ·æ¸©åº¦
            do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·
            
        Returns:
            ç”Ÿæˆçš„å›ç­”åˆ—è¡¨
        """
        # RoboBrainä¸»è¦å¤„ç†å›¾åƒï¼Œå¯¹äºè§†é¢‘æˆ‘ä»¬å–å…³é”®å¸§
        image_list_list = []
        for video_frames in video_list_list:
            if len(video_frames) > 0:
                # å–ä¸­é—´å¸§ä½œä¸ºä»£è¡¨
                mid_frame = video_frames[len(video_frames) // 2]
                image_list_list.append([mid_frame])
            else:
                image_list_list.append([])
        
        return self.generate_image(image_list_list, question_list, max_new_tokens, temperature, do_sample)
    
    def move_to_device(self, device):
        """ç§»åŠ¨æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡"""
        if self.model is not None and device != self.device:
            if self.is_model_parallel:
                print("âš ï¸  è­¦å‘Š: æ¨¡å‹å·²åˆ†å¸ƒåˆ°å¤šä¸ªGPUï¼Œæ— æ³•ç§»åŠ¨åˆ°å•ä¸ªè®¾å¤‡")
                print("å¦‚éœ€åˆ‡æ¢åˆ°å•å¡æ¨¡å¼ï¼Œè¯·é‡æ–°åˆå§‹åŒ–æ¨¡å‹")
            else:
                self.model = self.model.to(device)
                self.device = device
                print(f"æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
    
    def get_gpu_info(self):
        """è·å–GPUä¿¡æ¯"""
        if not torch.cuda.is_available():
            return "æ— å¯ç”¨GPU"
        
        info = []
        info.append(f"GPUæ•°é‡: {self.gpu_count}")
        
        for i in range(self.gpu_count):
            name = torch.cuda.get_device_name(i)
            memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            info.append(f"GPU {i}: {name} ({memory:.1f}GB)")
        
        if self.is_model_parallel:
            info.append("æ¨¡å¼: å¤šå¡å¹¶è¡Œ")
            if hasattr(self.model, 'hf_device_map'):
                info.append("è®¾å¤‡åˆ†é…:")
                for layer, device in list(self.model.hf_device_map.items())[:5]:
                    info.append(f"  {layer}: {device}")
                if len(self.model.hf_device_map) > 5:
                    info.append(f"  ... è¿˜æœ‰ {len(self.model.hf_device_map) - 5} å±‚")
        else:
            info.append(f"æ¨¡å¼: å•å¡ ({self.device})")
        
        return "\n".join(info)
    
    def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        if torch.cuda.is_available():
            # æ¸…ç†æ‰€æœ‰GPUçš„ç¼“å­˜
            for i in range(self.gpu_count):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()
        gc.collect()
        print("ç¼“å­˜å·²æ¸…ç†")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œæ¸…ç†èµ„æº"""
        self.clear_cache() 