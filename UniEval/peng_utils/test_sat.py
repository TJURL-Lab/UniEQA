import torch
import cv2
import numpy as np
from PIL import Image
import os
import sys
from typing import List, Union, Optional

# æ·»åŠ SATå’ŒLLaVAè·¯å¾„
sat_path = "/home/fx/Exp2/test/EmbodiedEval/SAT"
llava_path = os.path.join(sat_path, "models/LLaVA_modified/LLaVA")
sys.path.append(sat_path)
sys.path.append(llava_path)

# å°è¯•å¯¼å…¥peftï¼Œå¦‚æœç‰ˆæœ¬ä¸å…¼å®¹åˆ™ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ
try:
    from peft import PeftModel
    PEFT_AVAILABLE = True
except ImportError as e:
    print(f"Warning: PEFT import failed: {e}")
    PEFT_AVAILABLE = False
    # å°è¯•é™çº§å¤„ç†
    try:
        import subprocess
        import pkg_resources
        
        # æ£€æŸ¥å½“å‰ç‰ˆæœ¬
        current_accelerate = pkg_resources.get_distribution("accelerate").version
        current_peft = pkg_resources.get_distribution("peft").version
        
        print(f"Current versions - accelerate: {current_accelerate}, peft: {current_peft}")
        
        # å»ºè®®çš„å…¼å®¹ç‰ˆæœ¬
        print("Trying to install compatible versions...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "accelerate==0.21.0", "peft==0.4.0"])
        
        # é‡æ–°å¯¼å…¥
        from peft import PeftModel
        PEFT_AVAILABLE = True
        print("Successfully installed compatible versions")
        
    except Exception as install_error:
        print(f"Failed to install compatible versions: {install_error}")
        PEFT_AVAILABLE = False

from huggingface_hub import hf_hub_download

# ç°åœ¨å°è¯•å¯¼å…¥SATæ¨¡å‹
try:
    import models.model_interface as models
    print("Successfully imported SAT models")
except ImportError as e:
    print(f"Failed to import SAT models: {e}")
    print("Please ensure LLaVA is properly installed in the SAT directory")
    raise

from llava.mm_utils import (
    process_images,
    tokenizer_image_token,
    get_model_name_from_path,
    KeywordsStoppingCriteria,
    expand2square,
)
from llava.constants import (
    IMAGE_TOKEN_INDEX,
    DEFAULT_IMAGE_TOKEN,
    DEFAULT_IM_START_TOKEN,
    DEFAULT_IM_END_TOKEN,
    IMAGE_PLACEHOLDER,
)

class TestSAT:
    """SATæ¨¡å‹æ¨ç†ç±»ï¼Œæ”¯æŒå¤šå¡æ¨ç†å’Œå¤šæ¨¡æ€å¯¹è¯"""
    
    def __init__(self, device=None):
        """
        åˆå§‹åŒ–SATæ¨¡å‹
        
        Args:
            device: è®¾å¤‡å‚æ•°ï¼ˆå…¼å®¹peng_utilsæ¥å£ï¼‰
        """
        self.device = device
        self.dtype = torch.float32  # ä½¿ç”¨float32é¿å…Halfç²¾åº¦é—®é¢˜
        self.temperature = 0
        self.top_p = 1.0
        self.num_beams = 1
        self.max_new_tokens = 768
        self.checkpoint_name = None  # å¦‚æœæœ‰LoRAè·¯å¾„å¯è®¾ç½®
        self._init_model()
    
    def _init_model(self):
        """åˆå§‹åŒ–SATæ¨¡å‹"""
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                import gc
                gc.collect()
                
                # è®¾ç½®æ›´æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–
                torch.backends.cudnn.benchmark = False
                torch.backends.cudnn.deterministic = True
                
                # è®¾ç½®æ›´ä¿å®ˆçš„å†…å­˜åˆ†é…ç­–ç•¥
                os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:64'
                
                # æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                gpu_count = torch.cuda.device_count()
                print(f"ğŸ” æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
                for i in range(gpu_count):
                    total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                    allocated_memory = torch.cuda.memory_allocated(i) / 1024**3
                    free_memory = (torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_reserved(i)) / 1024**3
                    print(f"GPU {i}: æ€»å†…å­˜: {total_memory:.2f}GB, å·²åˆ†é…: {allocated_memory:.2f}GB, å¯ç”¨: {free_memory:.2f}GB")
            
            # å°è¯•å¯¼å…¥é‡åŒ–ç›¸å…³åº“
            try:
                import transformers
                from transformers import BitsAndBytesConfig
                QUANTIZATION_AVAILABLE = True
                print("Quantization libraries available")
            except ImportError:
                QUANTIZATION_AVAILABLE = False
                print("Quantization libraries not available, falling back to FP16")
            
            gpu_count = torch.cuda.device_count()
            print(f"Available GPUs: {gpu_count}")
            config = {
                    'temperature': self.temperature,
                    'top_p': self.top_p,
                    'num_beams': self.num_beams,
                'max_new_tokens': self.max_new_tokens,
                'output_hidden_states': False
            }
            from SAT.models import model_interface as models
            from transformers import AutoTokenizer
            from transformers import CLIPImageProcessor
            global PEFT_AVAILABLE
            try:
                from peft import PeftModel
                PEFT_AVAILABLE = True
            except ImportError:
                PEFT_AVAILABLE = False
            
            # å¤šå¡ - ä½¿ç”¨æ›´ä¿å®ˆçš„ç­–ç•¥
            if gpu_count > 1:
                print(f"Multiple GPUs detected ({gpu_count}), using conservative single-GPU mode")
                print("Loading base model on single GPU to avoid memory issues...")
                
                # å¼ºåˆ¶ä½¿ç”¨å•GPUæ¨¡å¼ï¼Œé¿å…å†…å­˜ä¸è¶³
                device_map = "cuda:0"  # åªä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
                
                # è®¾ç½®4bité‡åŒ–é…ç½®ï¼ˆä¼˜å…ˆï¼‰æˆ–8bité‡åŒ–é…ç½®
                if QUANTIZATION_AVAILABLE:
                    try:
                        print("Attempting 4bit quantization for maximum memory efficiency...")
                        quantization_config = BitsAndBytesConfig(
                            load_in_4bit=True,
                            bnb_4bit_compute_dtype=torch.float16,
                            bnb_4bit_use_double_quant=True,
                            bnb_4bit_quant_type="nf4",
                            llm_int8_threshold=6.0,
                            llm_int8_has_fp16_weight=False,
                        )
                        print("4bit quantization config created successfully")
                    except Exception as e:
                        print(f"4bit quantization failed: {e}, falling back to 8bit...")
                        try:
                            quantization_config = BitsAndBytesConfig(
                                load_in_8bit=True,
                                llm_int8_threshold=6.0,
                                llm_int8_has_fp16_weight=False,
                            )
                            print("8bit quantization config created successfully")
                        except Exception as e2:
                            print(f"8bit quantization also failed: {e2}, using FP16")
                            quantization_config = None
                else:
                    quantization_config = None
                    print("Using FP16 precision (no quantization)")
                
                # ä½¿ç”¨å•GPUåŠ è½½
                base_model = models.LlavaModel_13B_Interface(config, device_map=device_map)
            
                # åº”ç”¨é‡åŒ–åˆ°æ¨¡å‹
                if QUANTIZATION_AVAILABLE and quantization_config is not None:
                    print("Applying quantization to model...")
                    try:
                        base_model.model = base_model.model.quantize(quantization_config)
                        print("Quantization applied successfully")
                    except Exception as e:
                        print(f"Quantization application failed: {e}, continuing with unquantized model")
                
                if PEFT_AVAILABLE and self.checkpoint_name:
                    print("Loading LoRA model...")
                    try:
                        lora_model = PeftModel.from_pretrained(base_model, self.checkpoint_name)
                        # ä¿æŒé‡åŒ–ç²¾åº¦
                        if QUANTIZATION_AVAILABLE:
                            lora_model = lora_model.half()
                        print("Merging and unloading LoRA...")
                        self.model = lora_model.merge_and_unload()
                    except Exception as lora_error:
                        print(f"LoRA loading failed: {lora_error}")
                        print("Falling back to base model...")
                        self.model = base_model
                else:
                    self.model = base_model
                
                # å¼ºåˆ¶æ¨¡å‹ä½¿ç”¨float32é¿å…Halfç²¾åº¦é—®é¢˜
                try:
                    self.model = self.model.float()
                    print("âœ… æ¨¡å‹å·²è½¬æ¢ä¸ºfloat32ç²¾åº¦")
                except Exception as e:
                    print(f"âš ï¸ æ¨¡å‹ç²¾åº¦è½¬æ¢å¤±è´¥: {e}")
                
                print("SAT model loaded successfully with multi-GPU support")
            else:
                print(f"Loading SAT model on single device: {self.device}")
                base_model = models.LlavaModel_13B_Interface(config, device_map=None)
            
                # å•å¡ä¹Ÿåº”ç”¨é‡åŒ–
                if QUANTIZATION_AVAILABLE:
                    try:
                        print("Attempting 4bit quantization for single device...")
                        quantization_config = BitsAndBytesConfig(
                            load_in_4bit=True,
                            bnb_4bit_compute_dtype=torch.float16,
                            bnb_4bit_use_double_quant=True,
                            bnb_4bit_quant_type="nf4",
                            llm_int8_threshold=6.0,
                            llm_int8_has_fp16_weight=False,
                        )
                        base_model.model = base_model.model.quantize(quantization_config)
                        print("4bit quantization applied to single device model")
                    except Exception as e:
                        print(f"4bit quantization failed: {e}, trying 8bit...")
                        try:
                            quantization_config = BitsAndBytesConfig(
                                load_in_8bit=True,
                                llm_int8_threshold=6.0,
                                llm_int8_has_fp16_weight=False,
                            )
                            base_model.model = base_model.model.quantize(quantization_config)
                            print("8bit quantization applied to single device model")
                        except Exception as e2:
                            print(f"8bit quantization also failed: {e2}, using unquantized model")
                
                if PEFT_AVAILABLE and self.checkpoint_name:
                    print("Loading LoRA model...")
                    try:
                        lora_model = PeftModel.from_pretrained(base_model, self.checkpoint_name)
                        if QUANTIZATION_AVAILABLE:
                            lora_model = lora_model.half()
                        print("Merging and unloading LoRA...")
                        self.model = lora_model.merge_and_unload()
                    except Exception as lora_error:
                        print(f"LoRA loading failed: {lora_error}")
                        print("Falling back to base model...")
                        self.model = base_model
                else:
                    self.model = base_model
                
                # å¼ºåˆ¶æ¨¡å‹ä½¿ç”¨float32é¿å…Halfç²¾åº¦é—®é¢˜
                try:
                    self.model = self.model.float()
                    print("âœ… æ¨¡å‹å·²è½¬æ¢ä¸ºfloat32ç²¾åº¦")
                except Exception as e:
                    print(f"âš ï¸ æ¨¡å‹ç²¾åº¦è½¬æ¢å¤±è´¥: {e}")
                
                print("SAT model loaded successfully on single device")
            self.tokenizer = base_model.tokenizer
            self.image_processor = base_model.image_processor
            
            # æœ€ç»ˆå†…å­˜æ¸…ç†
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
                
        except Exception as e:
            print(f"Error loading SAT model: {e}")
            import traceback
            traceback.print_exc()
            raise e
    
    def move_to_device(self, device):
        # è°ƒè¯•ä¿¡æ¯
        print(f"move_to_device called with device: {device}")
        print(f"Model type: {type(self.model)}")
        print(f"Model has hf_device_map: {hasattr(self.model, 'hf_device_map')}")
        if hasattr(self.model, 'hf_device_map'):
            print(f"hf_device_map value: {self.model.hf_device_map}")
        
        # å¤šå¡æ¨¡å¼ä¸‹ä¸è¿ç§»
        if hasattr(self.model, 'hf_device_map') and self.model.hf_device_map is not None:
            print("Model is distributed across multiple GPUs, cannot move to single device")
            print(f"Current device map: {self.model.hf_device_map}")
            return
        
        # å…¼å®¹int/str/torch.device
        if isinstance(device, torch.device):
            device_str = device.type if device.index is None else f"{device.type}:{device.index}"
        else:
            device_str = str(device)
        if "cuda" in device_str:
            # ä½¿ç”¨FP32é¿å…Halfç²¾åº¦é—®é¢˜
            self.dtype = torch.float32
            self.device = device_str
            try:
                # å¼ºåˆ¶æ¨¡å‹ä½¿ç”¨float32
                self.model = self.model.float()
                self.model = self.model.to(device_str)
                print(f"Model moved to GPU: {device_str} with dtype: {self.dtype}")
            except torch.cuda.OutOfMemoryError as e:
                print(f"CUDA OOM when moving model to {device_str}: {e}")
                print("Model may already be on correct device or using quantization")
                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­ä½¿ç”¨
        else:
            self.dtype = torch.float32
            self.device = "cpu"
            try:
                self.model = self.model.to("cpu")
                print("Model moved to CPU")
            except Exception as e:
                print(f"Error moving model to CPU: {e}")
                print("Model may already be on correct device")
    
    def _load_image(self, image_input: Union[str, np.ndarray, Image.Image]) -> Image.Image:
        """åŠ è½½å›¾åƒä¸ºPILå›¾åƒ"""
        try:
            if isinstance(image_input, str):
                # æ–‡ä»¶è·¯å¾„
                if os.path.exists(image_input):
                    return Image.open(image_input).convert('RGB')
                else:
                    raise FileNotFoundError(f"Image file not found: {image_input}")
            elif isinstance(image_input, np.ndarray):
                # numpyæ•°ç»„
                return Image.fromarray(image_input.astype('uint8')).convert('RGB')
            elif isinstance(image_input, Image.Image):
                # PILå›¾åƒ
                return image_input.convert('RGB')
            else:
                raise ValueError(f"Unsupported image type: {type(image_input)}")
        except Exception as e:
            print(f"Error loading image: {e}")
            raise
    
    def _preprocess_images(self, images: List[Union[str, np.ndarray, Image.Image]]) -> torch.Tensor:
        """é¢„å¤„ç†å›¾åƒåˆ—è¡¨"""
        processed_images = []
        
        def flatten_image_list(image_input):
            """å°†ä»»æ„æ ¼å¼çš„å›¾åƒè¾“å…¥å±•å¹³ä¸ºPIL.Imageåˆ—è¡¨"""
            def _recursive_flatten(item, depth=0, max_depth=10):
                if depth > max_depth:
                    return []
                    
                if isinstance(item, list):
                    flat_images = []
                    for sub_item in item:
                        flat_images.extend(_recursive_flatten(sub_item, depth + 1, max_depth))
                    return flat_images
                elif isinstance(item, (int, float)):
                    return []
                else:
                    try:
                        loaded_img = self._load_image(item)
                        if loaded_img is not None:
                            return [loaded_img]
                        else:
                            return []
                    except Exception as e:
                        # å‡å°‘è°ƒè¯•è¾“å‡º
                        return []
            
            return _recursive_flatten(image_input)
        
        # å±•å¹³è¾“å…¥å›¾åƒåˆ—è¡¨
        flat_images = flatten_image_list(images)
        
        if not flat_images:
            print("No valid images found, creating default black image")
            # åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„é»‘è‰²å›¾åƒ
            default_image = Image.new('RGB', (224, 224), (0, 0, 0))
            flat_images = [default_image]
        
        for pil_image in flat_images:
            try:
                # æ‰©å±•ä¸ºæ­£æ–¹å½¢
                image = expand2square(pil_image, tuple(int(x*255) for x in self.image_processor.image_mean))
                
                # é¢„å¤„ç†
                image = self.image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]
                processed_images.append(image)
            except Exception as e:
                print(f"Error preprocessing image: {e}")
                # åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„é»‘è‰²å›¾åƒä½œä¸ºfallback
                default_image = Image.new('RGB', (224, 224), (0, 0, 0))
                image = expand2square(default_image, tuple(int(x*255) for x in self.image_processor.image_mean))
                image = self.image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]
                processed_images.append(image)
        
        # å †å ä¸ºbatch
        pixel_values = torch.stack(processed_images, dim=0)
        return pixel_values  # ä¸è¦ .to('cuda')
    
    def _preprocess_text(self, prompt: str) -> tuple:
        """é¢„å¤„ç†æ–‡æœ¬"""
        input_ids = []
        attention_mask = []
        
        # æ·»åŠ å›¾åƒtoken
        input_id = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt")
        input_ids.append(input_id)
        attention_mask.append(torch.ones_like(input_id))
        
        # å¡«å……åºåˆ—
        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)
        attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)
        
        return input_ids, attention_mask  # ä¸è¦ .to('cuda')
    
    def chat_image(self, 
                   image: Union[str, np.ndarray, Image.Image, List], 
                   question: str) -> str:
        """
        ä¸å›¾åƒè¿›è¡Œå¯¹è¯
        
        Args:
            image: å›¾åƒè¾“å…¥ï¼ˆå¯ä»¥æ˜¯è·¯å¾„ã€numpyæ•°ç»„ã€PILå›¾åƒæˆ–åˆ—è¡¨ï¼‰
            question: é—®é¢˜æ–‡æœ¬
        
        Returns:
            æ¨¡å‹å›ç­”
        """
        try:
            # å¤„ç†å•ä¸ªå›¾åƒæˆ–å›¾åƒåˆ—è¡¨
            if isinstance(image, list):
                return self._chat_multiple_images(image, question)
            else:
                return self._chat_single_image(image, question)
        except Exception as e:
            print(f"Error in chat_image: {e}")
            return f"Error: {str(e)}"
    
    def _chat_single_image(self, image: Union[str, np.ndarray, Image.Image], question: str) -> str:
        try:
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            pixel_values = self._preprocess_images([image])
            input_ids, attention_mask = self._preprocess_text(question)
            inputs = {
                "input_ids": input_ids,
                "attention_mask": attention_mask,
                'pixel_values': pixel_values,
            }
            
            # ç¡®ä¿è¾“å…¥ä½¿ç”¨æ­£ç¡®çš„dtypeï¼Œé¿å…Halfç²¾åº¦é—®é¢˜
            for key, value in inputs.items():
                if isinstance(value, torch.Tensor) and value.dtype.is_floating_point:
                    # ä½¿ç”¨float32é¿å…Halfç²¾åº¦é—®é¢˜
                    inputs[key] = value.to(torch.float32)
            
            # ä½¿ç”¨æ›´ä¿å®ˆçš„æ¨ç†è®¾ç½®
            with torch.no_grad():
                try:
                    # ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                    for key, value in inputs.items():
                        if isinstance(value, torch.Tensor):
                            # è·å–æ¨¡å‹è®¾å¤‡
                            model_device = next(self.model.parameters()).device
                            inputs[key] = value.to(model_device)
                    
                    # SATæ¨¡å‹çš„forwardæ–¹æ³•ä¸æ¥å—ç”Ÿæˆå‚æ•°ï¼Œç›´æ¥ä½¿ç”¨inputs
                    generated_ids = self.model(**inputs)
                    generated_ids[generated_ids == -200] = 1
                    generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
                    return generated_text[0] if generated_text else ""
                except torch.cuda.OutOfMemoryError as e:
                    print(f"CUDA OOM during inference: {e}")
                    # å°è¯•æ¸…ç†å†…å­˜å¹¶é‡è¯•
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        import gc
                        gc.collect()
                    
                    # ä½¿ç”¨æ›´å°çš„batch sizeé‡è¯•
                    try:
                        # å¦‚æœæœ‰å¤šå¼ å›¾åƒï¼Œåªä½¿ç”¨ç¬¬ä¸€å¼ 
                        if inputs['pixel_values'].shape[0] > 1:
                            inputs['pixel_values'] = inputs['pixel_values'][:1]
                            inputs['input_ids'] = inputs['input_ids'][:1]
                            inputs['attention_mask'] = inputs['attention_mask'][:1]
                        
                        # é‡è¯•æ—¶ä¹Ÿç›´æ¥ä½¿ç”¨inputs
                        generated_ids = self.model(**inputs)
                        generated_ids[generated_ids == -200] = 1
                        generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
                        return generated_text[0] if generated_text else ""
                    except Exception as e2:
                        print(f"Retry also failed: {e2}")
                        return f"Error: Memory insufficient for inference - {e2}"
                        
        except Exception as e:
            print(f"Error in _chat_single_image: {e}")
            return f"Error: {str(e)}"
    
    def _chat_multiple_images(self, images: List[Union[str, np.ndarray, Image.Image]], question: str) -> str:
        pixel_values = self._preprocess_images(images)
        input_ids, attention_mask = self._preprocess_text(question)
        inputs = {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            'pixel_values': pixel_values,
        }
        
        # ç¡®ä¿è¾“å…¥ä½¿ç”¨æ­£ç¡®çš„dtypeï¼Œé¿å…Halfç²¾åº¦é—®é¢˜
        for key, value in inputs.items():
            if isinstance(value, torch.Tensor) and value.dtype.is_floating_point:
                # ä½¿ç”¨float32é¿å…Halfç²¾åº¦é—®é¢˜
                inputs[key] = value.to(torch.float32)
        
        with torch.no_grad():
            # ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            for key, value in inputs.items():
                if isinstance(value, torch.Tensor):
                    # è·å–æ¨¡å‹è®¾å¤‡
                    model_device = next(self.model.parameters()).device
                    inputs[key] = value.to(model_device)
            
            # SATæ¨¡å‹çš„forwardæ–¹æ³•ä¸æ¥å—ç”Ÿæˆå‚æ•°ï¼Œç›´æ¥ä½¿ç”¨inputs
            generated_ids = self.model(**inputs)
            generated_ids[generated_ids == -200] = 1
            generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
        return generated_text[0] if generated_text else ""
    
    def generate_image(self, image, question):
        """ç”Ÿæˆå›¾åƒå›ç­”ï¼ˆå…¼å®¹peng_utilsæ¥å£ï¼‰"""
        print("generate begins")
        print(f"image:{type(image)}\nquestion:{type(question)}{question}")
        
        outputs = []
        image_list_list = image
        question_list = question
        
        for image_list, question in zip(image_list_list, question_list):
            try:
                # å¤„ç†å›¾åƒåˆ—è¡¨ï¼Œå–ç¬¬ä¸€å¼ å›¾åƒ
                if len(image_list) > 0:
                    img = image_list[0]
                    # ç›´æ¥è°ƒç”¨chat_imageå¤„ç†ï¼Œä¸é€šè¿‡get_image
                    output = self.chat_image(img, question)
                    outputs.append(output)
                else:
                    outputs.append("No image provided")
                    
            except Exception as e:
                print(f"Error processing image: {e}")
                outputs.append(f"Error: {str(e)}")
        
        print("generate finish")
        return outputs
    
    def batch_generate(self, image_list: List[Union[str, np.ndarray, Image.Image]], 
                      question_list: List[str]) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆå›ç­”
        
        Args:
            image_list: å›¾åƒåˆ—è¡¨
            question_list: é—®é¢˜åˆ—è¡¨
        
        Returns:
            å›ç­”åˆ—è¡¨
        """
        if len(image_list) != len(question_list):
            raise ValueError("Image list and question list must have the same length")
        
        answers = []
        for i, (image, question) in enumerate(zip(image_list, question_list)):
            try:
                answer = self.generate_image(image, question)
                answers.append(answer)
            except Exception as e:
                print(f"Error processing batch item {i}: {e}")
                answers.append(f"Error: {str(e)}")
        
        return answers


def test_sat():
    """æµ‹è¯•SATæ¨¡å‹"""
    print("=== Testing SAT Model with 4bit Quantization ===")
    
    # æ£€æŸ¥GPUå†…å­˜
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"Available GPUs: {gpu_count}")
        for i in range(gpu_count):
            total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            allocated_memory = torch.cuda.memory_allocated(i) / 1024**3
            free_memory = torch.cuda.memory_reserved(i) / 1024**3
            print(f"GPU {i}: Total: {total_memory:.2f}GB, Allocated: {allocated_memory:.2f}GB, Reserved: {free_memory:.2f}GB")
    
    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨å¤šå¡ï¼‰
    try:
        model = TestSAT(
            device="cuda:0" # æŒ‡å®šè®¾å¤‡
        )
    except Exception as e:
        print(f"Failed to initialize model: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æˆåŠŸåŠ è½½å¹¶é‡åŒ–
    print("\n--- Model Status Check ---")
    print(f"Model type: {type(model.model)}")
    if hasattr(model.model, 'hf_device_map'):
        print(f"Device map: {model.model.hf_device_map}")
    
    # æ£€æŸ¥æ¨¡å‹å‚æ•°æ˜¯å¦é‡åŒ–
    total_params = 0
    quantized_params = 0
    for name, param in model.model.named_parameters():
        total_params += param.numel()
        if hasattr(param, 'dtype'):
            if param.dtype in [torch.int8, torch.uint8]:
                quantized_params += param.numel()
    
    print(f"Total parameters: {total_params:,}")
    print(f"Quantized parameters: {quantized_params:,}")
    if total_params > 0:
        quantization_ratio = quantized_params / total_params * 100
        print(f"Quantization ratio: {quantization_ratio:.2f}%")
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_image = np.ones((224, 224, 3), dtype=np.uint8) * 128  # ç°è‰²å›¾åƒ
    
    # æµ‹è¯•å•ä¸ªå›¾åƒ
    print("\n--- Single Image Test ---")
    question = "What color is this image?"
    print(f"Question: {question}")
    
    try:
        answer = model.generate_image(test_image, question)
        print(f"Answer: {answer}")
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
    
    # æµ‹è¯•å¤šä¸ªå›¾åƒ
    print("\n--- Multiple Images Test ---")
    test_images = [
        np.ones((224, 224, 3), dtype=np.uint8) * 128,  # ç°è‰²
        np.ones((224, 224, 3), dtype=np.uint8) * 255,  # ç™½è‰²
    ]
    question = "What are the colors of these images?"
    print(f"Question: {question}")
    
    try:
        answer = model.generate_image(test_images, question)
        print(f"Answer: {answer}")
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
    
    # æµ‹è¯•æ‰¹é‡ç”Ÿæˆ
    print("\n--- Batch Generation Test ---")
    questions = ["What color is this?", "Is this image bright?"]
    print(f"Questions: {questions}")
    
    try:
        answers = model.batch_generate(test_images, questions)
        for i, answer in enumerate(answers):
            print(f"Answer {i+1}: {answer}")
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_sat() 