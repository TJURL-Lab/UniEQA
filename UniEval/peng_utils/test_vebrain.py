import numpy as np
from PIL import Image
from transformers import AutoModel, AutoTokenizer
import torch
from decord import VideoReader, cpu
import math

import os
from peng_utils import get_image, contact_img

from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info

class Testvebrain:
    def __init__(self, device=None):
        print("ğŸš€ Loading vebrain with optimizations...")
        
        # ç¡®å®šè®¾å¤‡
        if device is None:
            if torch.cuda.is_available():
                device = torch.device('cuda:0')
                self.dtype = torch.float16
            else:
                device = torch.device('cpu')
                self.dtype = torch.float32
        else:
            self.dtype = torch.float16 if 'cuda' in str(device) else torch.float32
        
        self.device = device
        print(f"ğŸ¯ Target device: {device}, dtype: {self.dtype}")
        
        # ä½¿ç”¨ä¼˜åŒ–çš„è®¾å¤‡æ˜ å°„ç›´æ¥åŠ è½½åˆ°GPU
        if torch.cuda.is_available() and torch.cuda.device_count() > 1:
            # å¤šGPUæƒ…å†µï¼šä½¿ç”¨è‡ªåŠ¨è®¾å¤‡æ˜ å°„
            print(f"ğŸ”¥ Multi-GPU setup detected: {torch.cuda.device_count()} GPUs")
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                "/home/fx/Exp2/test/EmbodiedEval/msjeval/VeBrain",
                torch_dtype=self.dtype,
                device_map='auto',  # è‡ªåŠ¨åˆ†å¸ƒåˆ°å¤šGPU
                low_cpu_mem_usage=True,
                trust_remote_code=True
            )
            self.is_multi_gpu = True
        else:
            # å•GPUæƒ…å†µï¼šç›´æ¥åŠ è½½åˆ°æŒ‡å®šè®¾å¤‡
            print(f"ğŸ“± Single GPU setup")
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                "/home/fx/Exp2/test/EmbodiedEval/msjeval/VeBrain",
                torch_dtype=self.dtype,
                low_cpu_mem_usage=True,
                trust_remote_code=True
            ).to(device)
            self.is_multi_gpu = False
        
        # å¤„ç†å™¨
        self.processor = AutoProcessor.from_pretrained(
            "/home/fx/Exp2/test/EmbodiedEval/msjeval/VeBrain"
        )
        
        # ä¼˜åŒ–å‚æ•°
        self.batch_size = 1  # å•æ ·æœ¬æ¨¡å¼ï¼Œæœ€ä½³ç¨³å®šæ€§
        self.max_new_tokens = 128
        
        print(f"âœ… Model loaded successfully! Multi-GPU: {self.is_multi_gpu}")

    def move_to_device(self, device=None):
        """ä¿æŒå…¼å®¹æ€§ï¼Œä½†å·²ç»åœ¨åˆå§‹åŒ–æ—¶ä¼˜åŒ–äº†è®¾å¤‡åŠ è½½"""
        if device is not None and not self.is_multi_gpu:
            if hasattr(device, 'type') and 'cuda' in device.type:
                self.dtype = torch.float16
                self.device = device
                self.model = self.model.to(device)
                print(f"Model moved to GPU: {device}")
            else:
                self.dtype = torch.float32
                self.device = 'cpu'
                self.model = self.model.to('cpu')
                print(f"Model moved to CPU")
        elif self.is_multi_gpu:
            print("Multi-GPU model already optimally distributed")

    def prepare_batch_inputs(self, image_list_list, question_list):
        """æ‰¹é‡å‡†å¤‡è¾“å…¥æ•°æ®"""
        batch_messages = []
        batch_texts = []
        batch_image_inputs = []
        batch_video_inputs = []
        
        for image_list, question in zip(image_list_list, question_list):
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": question},
                    ],
                }
            ]
            
            # æ·»åŠ å›¾åƒï¼Œé™åˆ¶å°ºå¯¸ä»¥æé«˜æ•ˆç‡
            for img in image_list:
                messages[0]["content"].append({
                    "type": "image",
                    "image": get_image(np.array(img)),
                    "resized_height": 224,
                    "resized_width": 224
                })
            
            # ç”Ÿæˆæ–‡æœ¬æ¨¡æ¿
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            # è°ƒè¯•è¾“å‡º - æ£€æŸ¥promptå†…å®¹ï¼ˆå¯é€‰ï¼‰
            # print(f"ğŸ” Debug prompt preview: {text[-200:] if len(text) > 200 else text}")
            
            # å¤„ç†è§†è§‰ä¿¡æ¯ - æ·»åŠ é”™è¯¯å¤„ç†
            try:
                image_inputs, video_inputs = process_vision_info(messages)
                # ç¡®ä¿è¿”å›å€¼ä¸ä¸ºNone
                image_inputs = image_inputs if image_inputs is not None else []
                video_inputs = video_inputs if video_inputs is not None else []
            except Exception as e:
                print(f"âš ï¸ Warning: process_vision_info failed: {e}")
                image_inputs, video_inputs = [], []
            
            batch_messages.append(messages)
            batch_texts.append(text)
            batch_image_inputs.extend(image_inputs)
            batch_video_inputs.extend(video_inputs)
        
        return batch_texts, batch_image_inputs, batch_video_inputs

    @torch.no_grad()  # æ·»åŠ æ— æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜
    def generate_image_batch(self, image_list_list, question_list):
        """æ‰¹é‡æ¨ç†ä¼˜åŒ–ç‰ˆæœ¬"""
        print(f"ğŸ”„ Batch inference: {len(image_list_list)} samples")
        
        all_outputs = []
        
        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜æº¢å‡º
        for i in range(0, len(image_list_list), self.batch_size):
            batch_images = image_list_list[i:i + self.batch_size]
            batch_questions = question_list[i:i + self.batch_size]
            
            print(f"  Processing batch {i//self.batch_size + 1}/{math.ceil(len(image_list_list)/self.batch_size)}")
            
            try:
                # å‡†å¤‡æ‰¹é‡è¾“å…¥
                batch_texts, batch_image_inputs, batch_video_inputs = self.prepare_batch_inputs(
                    batch_images, batch_questions
                )
                
                # å¤„ç†å™¨ç¼–ç  - ä¿®å¤ç©ºåˆ—è¡¨é—®é¢˜
                inputs = self.processor(
                    text=batch_texts,
                    images=batch_image_inputs if batch_image_inputs else None,
                    videos=None,  # æš‚æ—¶ç¦ç”¨videosé¿å…ç©ºåˆ—è¡¨é”™è¯¯
                    padding=True,
                    return_tensors="pt",
                )
                
                # å°†è¾“å…¥ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡ - ä¿®å¤è®¾å¤‡è­¦å‘Š
                inputs = inputs.to(self.device if not self.is_multi_gpu else 'cuda:0')
                
                # æ¸…ç†æ¨¡å‹çŠ¶æ€ - é‡è¦ï¼é˜²æ­¢æ‰¹æ¬¡é—´æ±¡æŸ“
                if hasattr(self.model, 'past_key_values'):
                    self.model.past_key_values = None
                if hasattr(self.model, '_past_key_values'):
                    self.model._past_key_values = None
                
                # å¼ºåˆ¶æ¸…ç†GPUç¼“å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # æ‰¹é‡æ¨ç† - ä¼˜åŒ–æ€§èƒ½å‚æ•°
                generated_ids = self.model.generate(
                    **inputs, 
                    max_new_tokens=self.max_new_tokens,
                    do_sample=False,  # ç¡®å®šæ€§æ¨ç†ï¼Œæ›´å¿«
                    num_beams=1,      # ä¸ä½¿ç”¨beam searchï¼Œæ›´å¿«
                    pad_token_id=self.processor.tokenizer.eos_token_id,
                    eos_token_id=self.processor.tokenizer.eos_token_id,
                    repetition_penalty=1.2,  # é™ä½é‡å¤æƒ©ç½šï¼ŒåŠ é€Ÿ
                    # no_repeat_ngram_size=3,  # æš‚æ—¶ç¦ç”¨ï¼Œæå‡é€Ÿåº¦
                    use_cache=True,  # é‡æ–°å¯ç”¨ç¼“å­˜ï¼ŒåŠ é€Ÿæ¨ç†
                )
                
                # è§£ç è¾“å‡º
                generated_ids_trimmed = [
                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                ]
                
                batch_outputs = self.processor.batch_decode(
                    generated_ids_trimmed, 
                    skip_special_tokens=True, 
                    clean_up_tokenization_spaces=False
                )
                
                # è°ƒè¯•å’Œæ¸…ç†è¾“å‡º
                cleaned_outputs = []
                for idx, output in enumerate(batch_outputs):
                    print(f"ğŸ” Raw output {idx}: '{output[:50]}...' (len: {len(output)})")
                    
                    # æ¸…ç†è¾“å‡º
                    output = output.strip()
                    
                    # æ£€æŸ¥å¹¶æ¸…ç†å¼‚å¸¸çš„addCriterionå‰ç¼€
                    if output.startswith('addCriterion'):
                        print(f"âš ï¸  Sample {idx}: Detected addCriterion prefix, removing...")
                        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ¢è¡Œç¬¦æˆ–è€…åˆç†çš„åˆ†å‰²ç‚¹
                        lines = output.split('\n')
                        if len(lines) > 1:
                            # è·³è¿‡ç¬¬ä¸€è¡Œï¼ˆaddCriterionè¡Œï¼‰
                            output = '\n'.join(lines[1:]).strip()
                        else:
                            # å¦‚æœæ²¡æœ‰æ¢è¡Œç¬¦ï¼Œå°è¯•æ‰¾åˆ°addCriterionåé¢çš„å†…å®¹
                            if len(output) > 12:  # "addCriterion" is 12 chars
                                output = output[12:].strip()
                        
                        print(f"âœ… Sample {idx}: Cleaned to: '{output[:50]}...'")
                    
                    if not output:
                        output = "Unable to process this question."
                    
                    cleaned_outputs.append(output)
                
                all_outputs.extend(cleaned_outputs)
                
                # æ¸…ç†GPUç¼“å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                print(f"âŒ Error in batch {i//self.batch_size + 1}: {e}")
                # é™çº§åˆ°å•ä¸ªæ ·æœ¬å¤„ç†
                for j, (img_list, question) in enumerate(zip(batch_images, batch_questions)):
                    try:
                        output = self.generate_image_single(img_list, question)
                        all_outputs.append(output)
                    except Exception as e2:
                        print(f"âŒ Error in single sample {i+j}: {e2}")
                        all_outputs.append(f"Error: {str(e2)}")
        
        print(f"âœ… Batch inference completed: {len(all_outputs)} outputs")
        return all_outputs

    def generate_image_single(self, image_list, question):
        """å•ä¸ªæ ·æœ¬æ¨ç†ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": question},
                ],
            }
        ]
        
        for img in image_list:
            messages[0]["content"].append({
                "type": "image",
                "image": get_image(np.array(img)),
                "resized_height": 224,
                "resized_width": 224
            })
        
        text = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        # è°ƒè¯•è¾“å‡º - æ£€æŸ¥å•ä¸ªæ ·æœ¬promptå†…å®¹ï¼ˆå¯é€‰ï¼‰
        # print(f"ğŸ” Single debug prompt preview: {text[-200:] if len(text) > 200 else text}")
        
        try:
            image_inputs, video_inputs = process_vision_info(messages)
            image_inputs = image_inputs if image_inputs is not None else []
            video_inputs = video_inputs if video_inputs is not None else []
        except Exception as e:
            print(f"âš ï¸ Warning: process_vision_info failed: {e}")
            image_inputs, video_inputs = [], []
        inputs = self.processor(
            text=[text],
            images=image_inputs if image_inputs else None,
            videos=None,  # æš‚æ—¶ç¦ç”¨videosé¿å…ç©ºåˆ—è¡¨é”™è¯¯
            padding=True,
            return_tensors="pt",
        )
        
        # å°†è¾“å…¥ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡ - ä¿®å¤è®¾å¤‡è­¦å‘Š
        inputs = inputs.to(self.device if not self.is_multi_gpu else 'cuda:0')
        
        # æ¸…ç†æ¨¡å‹çŠ¶æ€ - å•ä¸ªæ ·æœ¬ä¹Ÿéœ€è¦
        if hasattr(self.model, 'past_key_values'):
            self.model.past_key_values = None
        if hasattr(self.model, '_past_key_values'):
            self.model._past_key_values = None
        
        # å¼ºåˆ¶æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        generated_ids = self.model.generate(
            **inputs, 
            max_new_tokens=self.max_new_tokens,
            do_sample=False,  # ç¡®å®šæ€§æ¨ç†
            num_beams=1,      # ä¸ä½¿ç”¨beam search
            pad_token_id=self.processor.tokenizer.eos_token_id,
            eos_token_id=self.processor.tokenizer.eos_token_id,
            repetition_penalty=1.2,  # é™ä½é‡å¤æƒ©ç½šï¼ŒåŠ é€Ÿ
            # no_repeat_ngram_size=3,  # æš‚æ—¶ç¦ç”¨ï¼Œæå‡é€Ÿåº¦
            use_cache=True,  # é‡æ–°å¯ç”¨ç¼“å­˜ï¼ŒåŠ é€Ÿæ¨ç†
        )
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        output = self.processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]
        
        # è°ƒè¯•å’Œæ¸…ç†å•ä¸ªæ ·æœ¬è¾“å‡ºï¼ˆé™é»˜æ¨¡å¼ï¼‰
        # print(f"ğŸ” Single output raw: '{output[:50]}...' (len: {len(output)})")
        
        output = output.strip()
        
        # é™é»˜æ£€æŸ¥å¹¶æ¸…ç†å¼‚å¸¸çš„addCriterionå‰ç¼€
        if output.startswith('addCriterion'):
            # print("âš ï¸  Single sample: Detected addCriterion prefix, removing...")
            lines = output.split('\n')
            if len(lines) > 1:
                output = '\n'.join(lines[1:]).strip()
            else:
                if len(output) > 12:
                    output = output[12:].strip()
            # print(f"âœ… Single sample: Cleaned to: '{output[:50]}...'")
        
        if not output:
            output = "Unable to process this question."
        
        return output

    def generate_image(self, image_list_list, question_list):
        """ä¸»è¦æ¥å£ï¼šè‡ªåŠ¨é€‰æ‹©æ‰¹é‡æˆ–å•ä¸ªæ¨ç†"""
        print("ğŸš€ vebrain inference begins")
        print(f"ğŸ“Š Input: {len(image_list_list)} samples")
        
        if len(image_list_list) > 1:
            # ä½¿ç”¨æ‰¹é‡æ¨ç†
            outputs = self.generate_image_batch(image_list_list, question_list)
        else:
            # å•ä¸ªæ ·æœ¬
            outputs = [self.generate_image_single(image_list_list[0], question_list[0])]
        
        print("âœ… vebrain inference finished")
        return outputs 