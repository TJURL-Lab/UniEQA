import os
import sys
import warnings
import copy
import torch
import numpy as np
from PIL import Image
from decord import VideoReader, cpu
import requests
from io import BytesIO

warnings.filterwarnings("ignore")

from peng_utils import contact_img, get_image

sys.path.append("./peng_utils")
sys.path.insert(0, "/home/fx/Exp2/video_model/LLaVA-NeXT")

# LLaVA imports
from llava.model.builder import load_pretrained_model
from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token
from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IGNORE_INDEX
from llava.conversation import conv_templates, SeparatorStyle
from llava.utils import disable_torch_init

def load_image(image_file):
    if image_file.startswith('http') or image_file.startswith('https'):
        response = requests.get(image_file)
        image = Image.open(BytesIO(response.content)).convert('RGB')
    else:
        image = Image.open(image_file).convert('RGB')
    return image

def load_video(video_path, fps=1):
    vr = VideoReader(video_path, ctx=cpu(0))
    fps = round(vr.get_avg_fps()/fps)
    frame_idx = [i for i in range(0, len(vr), fps)]
    spare_frames = vr.get_batch(frame_idx).asnumpy()
    return spare_frames

class TestLLaVAOneVision:
    def __init__(self, device=None) -> None:
        disable_torch_init()
        
        # åŸºäºå®˜æ–¹æ•™ç¨‹çš„é…ç½®
        model_path = "/home/fx/Exp2/test/EmbodiedEval/msjeval/model_zoo/llava-onevision-qwen2-0.5b-ov"
        model_name = "llava_qwen"  # å®˜æ–¹æŒ‡å®šçš„æ¨¡å‹åç§°
        device_map = "auto"
        
        # æ˜¾å¼ç¦ç”¨ FlashAttention2
        llava_model_args = {
            "multimodal": True,
            "attn_implementation": None,  # ä½¿ç”¨eager attentioné¿å…flash attention
            "torch_dtype": torch.float16,
        }
        
        self.device = device if device is not None else "cuda"
        
        print(f"ğŸ”„ å¼€å§‹åŠ è½½LLaVA-OneVisionæ¨¡å‹: {model_path}")
        print(f"ä½¿ç”¨é…ç½® - æ¨¡å‹åç§°: {model_name}, attention: None")
        
        try:
            # å…ˆæ£€æŸ¥æ¨¡å‹çš„å®é™…åç§°
            detected_model_name = get_model_name_from_path(model_path)
            print(f"æ£€æµ‹åˆ°çš„æ¨¡å‹åç§°: {detected_model_name}")
            
            # ä½¿ç”¨æ£€æµ‹åˆ°çš„æ¨¡å‹åç§°
            self.tokenizer, self.model, self.image_processor, self.max_length = load_pretrained_model(
                model_path, 
                None, 
                detected_model_name,  # ä½¿ç”¨æ£€æµ‹åˆ°çš„æ¨¡å‹åç§°
                device_map=device_map, 
                **llava_model_args
            )
            
            self.model.eval()
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
            
        except Exception as e:
            print(f"âŒ ç¬¬ä¸€æ¬¡åŠ è½½å¤±è´¥: {e}")
            # å°è¯•æœ€ç®€å•çš„åŠ è½½æ–¹å¼
            try:
                print("ğŸ”„ å°è¯•æœ€ç®€å•çš„åŠ è½½æ–¹å¼...")
                # ä½¿ç”¨åŸºæœ¬çš„ llava æ¨¡å‹åç§°å’Œç®€å•å‚æ•°
                simple_args = {
                    "multimodal": True,
                    "attn_implementation": None,  # ä¿æŒä¸ºNone
                    "torch_dtype": torch.float16,
                }
                self.tokenizer, self.model, self.image_processor, self.max_length = load_pretrained_model(
                    model_path, 
                    None, 
                    "llava",  # ä½¿ç”¨åŸºæœ¬çš„llavaåç§°
                    device_map=None,  # ä¸ä½¿ç”¨device_map
                    **simple_args
                )
                self.model.eval()
                print("âœ… ç®€å•æ–¹å¼åŠ è½½æˆåŠŸ!")
            except Exception as e2:
                print(f"âŒ æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥: {e2}")
                raise e2
        
        # å¯¹è¯æ¨¡æ¿é…ç½®
        self.conv_template = "qwen_1_5"  # å®˜æ–¹æŒ‡å®šçš„å¯¹è¯æ¨¡æ¿
        
        # å…¶ä»–é…ç½®
        self.temperature = 0.2
        self.max_new_tokens = 512
        self.top_p = 0.7

    def move_to_device(self, device):
        """å…¼å®¹æ€§æ–¹æ³•ï¼Œå®é™…ä¸Šæ¨¡å‹å·²ç»é€šè¿‡device_mapåŠ è½½"""
        self.device = device
        pass

    def __device__(self):
        return self.device

    @torch.no_grad()
    def generate_image(self, image_list_list, question_list):
        outputs = []
        
        # ä½¿ç”¨å’Œ test_llava.py ç›¸åŒçš„æ–¹å¼é¢„å¤„ç†å›¾åƒ
        processed_images = contact_img(image_list_list)
        
        for image, question in zip(processed_images, question_list):
            try:
                # è½¬æ¢å›¾åƒæ ¼å¼ - æ¨¡ä»¿ test_llava.py çš„æ–¹å¼
                img = get_image(image)
                images = [img]  # LLaVA-OneVision é€šå¸¸å¤„ç†å•ä¸ªå›¾åƒ
                
                # ä½¿ç”¨å®˜æ–¹çš„process_imagesæ–¹æ³•
                image_tensors = process_images(images, self.image_processor, self.model.config)
                image_tensors = [_image.to(dtype=torch.float16, device=self.device) for _image in image_tensors]
                
                # å‡†å¤‡å¯¹è¯ï¼Œä½¿ç”¨å®˜æ–¹çš„æ–¹å¼
                conv = copy.deepcopy(conv_templates[self.conv_template])
                
                # æ„å»ºé—®é¢˜ï¼Œæ ¹æ®å›¾åƒæ•°é‡è°ƒæ•´
                if len(images) == 1:
                    question_with_image = DEFAULT_IMAGE_TOKEN + "\n" + question
                else:
                    # å¤šå›¾åƒæƒ…å†µ
                    image_tokens = " ".join([DEFAULT_IMAGE_TOKEN] * len(images))
                    question_with_image = image_tokens + "\n" + question
                
                conv.append_message(conv.roles[0], question_with_image)
                conv.append_message(conv.roles[1], None)
                prompt_question = conv.get_prompt()
                
                # Tokenize
                input_ids = tokenizer_image_token(
                    prompt_question, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt"
                ).unsqueeze(0).to(self.device)
                
                # å›¾åƒå°ºå¯¸
                image_sizes = [image.size for image in images]
                
                # ç”Ÿæˆå›ç­” - ä½¿ç”¨å®˜æ–¹çš„å‚æ•°
                with torch.inference_mode():
                    output_ids = self.model.generate(
                        input_ids,
                        images=image_tensors,
                        image_sizes=image_sizes,
                        do_sample=False,
                        temperature=0,
                        max_new_tokens=self.max_new_tokens,
                    )
                
                # è§£ç è¾“å‡º
                text_outputs = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)
                output = text_outputs[0].strip()
                
                # æ¸…ç†è¾“å‡ºï¼ˆç§»é™¤promptéƒ¨åˆ†ï¼‰
                if prompt_question in output:
                    output = output.replace(prompt_question, "").strip()
                
                outputs.append(output)
                
                print(f"âœ… å›¾åƒ {len(outputs)}: {question[:50]}... -> {output[:100]}...")
                
            except Exception as e:
                print(f"âŒ å›¾åƒç”Ÿæˆå¤±è´¥: {e}")
                outputs.append(f"Error processing image: {str(e)}")
        
        print("\n", {"prompt": question_list, "outputs": outputs}, "\n")
        return outputs

    @torch.no_grad()
    def generate_clip(self, files_list, question_list):
        outputs = []
        
        for video_file, question in zip(files_list, question_list):
            try:
                # åŠ è½½è§†é¢‘å¸§æˆ–å›¾åƒåˆ—è¡¨
                if isinstance(video_file, str):
                    # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼ŒåŠ è½½è§†é¢‘
                    video_frames = load_video(video_file)
                    # è½¬æ¢ä¸ºPILå›¾åƒ
                    images = [Image.fromarray(frame) for frame in video_frames]
                else:
                    # å¦‚æœå·²ç»æ˜¯å›¾åƒåˆ—è¡¨ï¼Œä½¿ç”¨å’Œ test_llava.py ç›¸åŒçš„å¤„ç†æ–¹å¼
                    # å¯¹äºè§†é¢‘æ–‡ä»¶ï¼Œä¹Ÿå¯èƒ½éœ€è¦ä½¿ç”¨ contact_img å¤„ç†
                    processed_images = contact_img([video_file])  # åŒ…è£…æˆéœ€è¦çš„æ ¼å¼
                    img = get_image(processed_images[0])
                    images = [img]
                
                # ä½¿ç”¨å®˜æ–¹çš„process_imagesæ–¹æ³•
                image_tensors = process_images(images, self.image_processor, self.model.config)
                image_tensors = [_image.to(dtype=torch.float16, device=self.device) for _image in image_tensors]
                
                # å‡†å¤‡å¯¹è¯
                conv = copy.deepcopy(conv_templates[self.conv_template])
                
                # å¯¹äºè§†é¢‘ï¼Œä½¿ç”¨å•ä¸ªå›¾åƒtokenï¼ˆLLaVA-OneVisionæ”¯æŒè§†é¢‘ç†è§£ï¼‰
                question_with_image = DEFAULT_IMAGE_TOKEN + "\n" + question
                
                conv.append_message(conv.roles[0], question_with_image)
                conv.append_message(conv.roles[1], None)
                prompt_question = conv.get_prompt()
                
                # Tokenize
                input_ids = tokenizer_image_token(
                    prompt_question, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors="pt"
                ).unsqueeze(0).to(self.device)
                
                # å›¾åƒå°ºå¯¸
                image_sizes = [image.size for image in images]
                
                # ç”Ÿæˆå›ç­”
                with torch.inference_mode():
                    output_ids = self.model.generate(
                        input_ids,
                        images=image_tensors,
                        image_sizes=image_sizes,
                        do_sample=False,
                        temperature=0,
                        max_new_tokens=self.max_new_tokens,
                    )
                
                # è§£ç è¾“å‡º
                text_outputs = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)
                output = text_outputs[0].strip()
                
                # æ¸…ç†è¾“å‡ºï¼ˆç§»é™¤promptéƒ¨åˆ†ï¼‰
                if prompt_question in output:
                    output = output.replace(prompt_question, "").strip()
                
                outputs.append(output)
                
                print(f"âœ… è§†é¢‘ {len(outputs)}: {question[:50]}... -> {output[:100]}...")
                
            except Exception as e:
                print(f"âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥: {e}")
                outputs.append(f"Error processing video: {str(e)}")
        
        print("\n", {"prompt": question_list, "outputs": outputs}, "\n")
        return outputs 