import os
import sys
import torch
from PIL import Image
import logging

# æ·»åŠ SpaceMantisè·¯å¾„åˆ°Pythonè·¯å¾„
spacemantis_path = "/home/fx/Exp2/video_model/SpaceMantis"
if spacemantis_path not in sys.path:
    sys.path.insert(0, spacemantis_path)
    print(f"âœ… Added {spacemantis_path} to Python path")

# ç›´æ¥å¯¼å…¥SpaceMantisçš„mllavaæ¨¡å—
from models.mllava import MLlavaProcessor, LlavaForConditionalGeneration, chat_mllava

class TestSpatialVLM:
    def __init__(self, device=None):
        # ä½¿ç”¨æœ¬åœ°SpaceMantisæ¨¡å‹
        model_path = "/home/sdd/fx/SpaceMantis/"
        
        print(f"Loading SpatialVLM (SpaceMantis) from: {model_path}")
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        
        try:
            # è®¾ç½®è®¾å¤‡
            if device is None:
                if torch.cuda.is_available():
                    device = "cuda"
                    self.dtype = torch.float16
                else:
                    device = "cpu"
                    self.dtype = torch.float32
            else:
                self.dtype = torch.float16 if 'cuda' in str(device) else torch.float32
            
            self.device = device
            print(f"ğŸ¯ Target device: {device}, dtype: {self.dtype}")
            
            # å®Œå…¨æŒ‰ç…§åŸå§‹ä»£ç åŠ è½½
            attn_implementation = None  # or "flash_attention_2"
            
            print("ğŸ“¦ Loading processor...")
            self.processor = MLlavaProcessor.from_pretrained(model_path)
            
            print("ğŸ”„ Loading model...")
            if torch.cuda.is_available() and torch.cuda.device_count() > 1:
                # å¤šGPU - ä½¿ç”¨device_map="auto"
                self.model = LlavaForConditionalGeneration.from_pretrained(
                    model_path, 
                    device_map="auto", 
                    torch_dtype=self.dtype, 
                    attn_implementation=attn_implementation
                )
                self.is_multi_gpu = True
            else:
                # å•GPU - ä½¿ç”¨device_map="cuda"
                self.model = LlavaForConditionalGeneration.from_pretrained(
                    model_path, 
                    device_map=device, 
                    torch_dtype=self.dtype, 
                    attn_implementation=attn_implementation
                )
                self.is_multi_gpu = False
            
            print(f"âœ… Model loaded successfully!")
            
            # å®Œå…¨æŒ‰ç…§åŸå§‹ä»£ç çš„ç”Ÿæˆå‚æ•°
            self.generation_kwargs = {
                "max_new_tokens": 1024,
                "num_beams": 1,
                "do_sample": False
            }
            
        except Exception as e:
            raise RuntimeError(f"Failed to initialize SpatialVLM: {str(e)}")

    def move_to_device(self, device=None):
        """ç§»åŠ¨æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆå…¼å®¹model_workerè¦æ±‚ï¼‰"""
        if device is not None and not self.is_multi_gpu:
            self.device = device
            try:
                if 'cuda' in str(device):
                    self.dtype = torch.float16
                    self.model = self.model.to(device)
                    print(f"âœ… SpatialVLM model moved to GPU: {device}")
                else:
                    self.dtype = torch.float32
                    self.model = self.model.to('cpu')
                    print(f"âœ… SpatialVLM model moved to CPU")
            except Exception as e:
                print(f"âš ï¸ Warning: Could not move SpatialVLM model to device {device}: {e}")
        elif self.is_multi_gpu:
            print("â„¹ï¸ Multi-GPU SpatialVLM model already optimally distributed")
        else:
            print(f"â„¹ï¸ SpatialVLM model already on device: {self.device}")

    def run_inference_single(self, image, content):
        """
        æŒ‰ç…§åŸå§‹ä»£ç çš„æ¨ç†æ–¹å¼ - ç®€åŒ–å›¾åƒç±»å‹æ£€æŸ¥
        """
        try:
            print(f"ğŸ¤– Running inference...")
            print(f"ğŸ“ Question: {content}")
            print(f"ğŸ–¼ï¸ Image type: {type(image)}")
            
            # ç¡®ä¿æ˜¯PIL Imageå¯¹è±¡
            if hasattr(image, 'size'):
                print(f"ğŸ–¼ï¸ Image size: {image.size}")
            else:
                raise ValueError(f"Expected PIL Image, got {type(image)}")
            
            # å®Œå…¨æŒ‰ç…§åŸå§‹ä»£ç ï¼šç›´æ¥ä¼ é€’å›¾åƒåˆ—è¡¨ç»™chat_mllava
            images = [image]
            
            # ä½¿ç”¨åŸå§‹çš„chat_mllavaå‡½æ•°
            response, history = chat_mllava(
                content,           # text
                images,           # images 
                self.model,       # model
                self.processor,   # processor
                **self.generation_kwargs
            )
            
            print(f"ğŸ“„ Response: {response[:100]}{'...' if len(response) > 100 else ''}")
            return response
                
        except Exception as e:
            error_msg = f"SpatialVLM inference failed: {str(e)}"
            print(f"âŒ {error_msg}")
            import traceback
            print(f"ğŸ” Traceback: {traceback.format_exc()}")
            return error_msg

    def process_multiple_images(self, image_list):
        """å¤„ç†å¤šå¼ å›¾åƒï¼Œè¿”å›ç»“æœåˆ—è¡¨"""
        results = []
        for i, image in enumerate(image_list):
            try:
                print(f"ğŸ”„ Processing image {i+1}/{len(image_list)}")
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ‰¹é‡å¤„ç†é€»è¾‘ï¼Œæš‚æ—¶é€ä¸ªå¤„ç†
                result = self.run_inference_single(image, "Describe this image.")
                results.append(result)
            except Exception as e:
                print(f"âŒ Error processing image {i+1}: {e}")
                results.append(f"Error: {str(e)}")
        return results

    @torch.no_grad()
    def generate_image(self, image_list_list, question_list):
        """
        å¯¹å›¾åƒåˆ—è¡¨è¿›è¡Œæ¨ç†ï¼ˆEmbodiedEvalæ¥å£ï¼‰
        æŒ‰ç…§test_llava.pyçš„å¤„ç†æ–¹å¼
        """
        try:
            print(f"ğŸ”¥ SpatialVLM batch image generation starting...")
            print(f"ğŸ“Š Processing {len(image_list_list)} image groups")
            
            # å¯¼å…¥contact_imgå’Œget_imageå‡½æ•°ï¼ˆå‚è€ƒtest_llava.pyï¼‰
            try:
                from . import contact_img, get_image
            except ImportError:
                import sys
                import os
                parent_dir = os.path.dirname(os.path.abspath(__file__))
                sys.path.insert(0, parent_dir)
                import __init__
                contact_img = __init__.contact_img
                get_image = __init__.get_image
            
            # ä½¿ç”¨contact_imgé¢„å¤„ç†å›¾åƒæ•°æ®ï¼ˆå‚è€ƒtest_llava.pyï¼‰
            contacted_images = contact_img(image_list_list)
            print(f"ğŸ“· After contact_img: {len(contacted_images)} images")
            
            results = []
            
            for i, (contacted_image, question) in enumerate(zip(contacted_images, question_list)):
                try:
                    print(f"\nğŸ”„ Processing group {i+1}/{len(contacted_images)}")
                    print(f"ğŸ“ Question: {question}")
                    print(f"ğŸ–¼ï¸ Contacted image type: {type(contacted_image)}")
                    
                    # ä½¿ç”¨get_imageå¤„ç†contacted_imageï¼ˆå‚è€ƒtest_llava.pyï¼‰
                    pil_image = get_image(contacted_image)
                    print(f"ğŸ–¼ï¸ After get_image: {type(pil_image)}")
                    
                    if hasattr(pil_image, 'size'):
                        print(f"ğŸ–¼ï¸ Final image size: {pil_image.size}")
                    
                    # è¿è¡Œæ¨ç†
                    response = self.run_inference_single(pil_image, question)
                    results.append(response)
                    
                except Exception as e:
                    error_msg = f"Error processing group {i+1}: {str(e)}"
                    print(f"âŒ {error_msg}")
                    import traceback
                    print(f"ğŸ” Traceback: {traceback.format_exc()}")
                    results.append(error_msg)
            
            print(f"âœ… SpatialVLM batch processing completed!")
            return results
            
        except Exception as e:
            print(f"âŒ SpatialVLM batch generation failed: {e}")
            import traceback
            print(f"ğŸ” Traceback: {traceback.format_exc()}")
            return [f"Batch processing error: {str(e)}"] * len(image_list_list)

    @torch.no_grad()
    def generate_clip(self, file_list, question_list):
        """
        å¯¹è§†é¢‘æ–‡ä»¶è¿›è¡Œæ¨ç†ï¼ˆEmbodiedEvalæ¥å£ï¼‰
        """
        try:
            print(f"ğŸ¬ SpatialVLM video generation starting...")
            print(f"ğŸ“Š Processing {len(file_list)} videos")
            
            results = []
            
            for i, (video_path, question) in enumerate(zip(file_list, question_list)):
                try:
                    print(f"\nğŸ”„ Processing video {i+1}/{len(file_list)}")
                    print(f"ğŸ¬ Video: {video_path}")
                    print(f"ğŸ“ Question: {question}")
                    
                    # ä»è§†é¢‘æå–å…³é”®å¸§
                    frames = self.extract_video_frames(video_path)
                    
                    if not frames:
                        print("âš ï¸ No frames extracted from video")
                        results.append("Could not extract frames from video")
                        continue
                    
                    # ä½¿ç”¨ç¬¬ä¸€å¸§è¿›è¡Œæ¨ç†
                    primary_frame = frames[0]
                    print(f"ğŸ–¼ï¸ Using first frame from {len(frames)} frames")
                    
                    # ä¿®æ”¹é—®é¢˜ä»¥é€‚åº”è§†é¢‘å†…å®¹
                    video_question = f"This is a frame from a video. {question}"
                    
                    response = self.run_inference_single(primary_frame, video_question)
                    results.append(response)
                    
                except Exception as e:
                    error_msg = f"Error processing video {i+1}: {str(e)}"
                    print(f"âŒ {error_msg}")
                    results.append(error_msg)
            
            print(f"âœ… SpatialVLM video processing completed!")
            return results
            
        except Exception as e:
            print(f"âŒ SpatialVLM video generation failed: {e}")
            return [f"Video processing error: {str(e)}"] * len(file_list)

    def extract_video_frames(self, video_path, num_frames=8):
        """ä»è§†é¢‘ä¸­æå–å…³é”®å¸§"""
        try:
            import cv2
            
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError(f"Cannot open video: {video_path}")
            
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            if total_frames == 0:
                raise ValueError("Video has no frames")
            
            # å‡åŒ€é‡‡æ ·å¸§
            frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]
            frames = []
            
            for frame_idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if ret:
                    # è½¬æ¢BGRåˆ°RGB
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    pil_image = Image.fromarray(frame_rgb)
                    frames.append(pil_image)
            
            cap.release()
            print(f"ğŸ“½ï¸ Extracted {len(frames)} frames from video")
            return frames
            
        except Exception as e:
            print(f"âŒ Frame extraction failed: {e}")
            return [] 