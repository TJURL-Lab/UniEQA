import numpy as np
from PIL import Image
from transformers import AutoModel, AutoTokenizer
import torch
from decord import VideoReader, cpu
import math
import traceback

import os
from peng_utils import get_image, contact_img

from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info

class Testqwen2_5_7b_fixed:
    def __init__(self, device=None):
        print("ğŸš€ Loading Qwen2.5-VL-7B-Instruct with optimizations and fixes...")
        
        # ç¡®å®šè®¾å¤‡
        if device is None:
            if torch.cuda.is_available():
                device = torch.device('cuda:0')
                self.dtype = torch.float16
            else:
                device = torch.device('cpu')
                self.dtype = torch.float32
        else:
            self.dtype = torch.float16 if 'cuda' in str(device) else torch.float32
        
        self.device = device
        print(f"ğŸ¯ Target device: {device}, dtype: {self.dtype}")
        
        # ä½¿ç”¨ä¼˜åŒ–çš„è®¾å¤‡æ˜ å°„ç›´æ¥åŠ è½½åˆ°GPU
        if torch.cuda.is_available() and torch.cuda.device_count() > 1:
            # å¤šGPUæƒ…å†µï¼šä½¿ç”¨è‡ªåŠ¨è®¾å¤‡æ˜ å°„
            print(f"ğŸ”¥ Multi-GPU setup detected: {torch.cuda.device_count()} GPUs")
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                "/home/fx/Exp2/test/EmbodiedEval/msjeval/model_zoo/Qwen2.5-VL-7B-Instruct",
                torch_dtype=self.dtype,
                device_map='auto',  # è‡ªåŠ¨åˆ†å¸ƒåˆ°å¤šGPU
                low_cpu_mem_usage=True,
                trust_remote_code=True
            )
            self.is_multi_gpu = True
        else:
            # å•GPUæƒ…å†µï¼šç›´æ¥åŠ è½½åˆ°æŒ‡å®šè®¾å¤‡
            print(f"ğŸ“± Single GPU setup")
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                "/home/fx/Exp2/test/EmbodiedEval/msjeval/model_zoo/Qwen2.5-VL-7B-Instruct",
                torch_dtype=self.dtype,
                low_cpu_mem_usage=True,
                trust_remote_code=True
            ).to(device)
            self.is_multi_gpu = False
        
        # å¤„ç†å™¨
        self.processor = AutoProcessor.from_pretrained(
            "/home/fx/Exp2/test/EmbodiedEval/msjeval/model_zoo/Qwen2.5-VL-7B-Instruct"
        )
        
        # ä¼˜åŒ–å‚æ•°
        self.batch_size = 4  # é™ä½æ‰¹é‡å¤§å°é¿å…OOM
        self.max_new_tokens = 128
        
        print(f"âœ… Model loaded successfully! Multi-GPU: {self.is_multi_gpu}")

    def move_to_device(self, device=None):
        """ä¿æŒå…¼å®¹æ€§ï¼Œä½†å·²ç»åœ¨åˆå§‹åŒ–æ—¶ä¼˜åŒ–äº†è®¾å¤‡åŠ è½½"""
        if device is not None and not self.is_multi_gpu:
            if hasattr(device, 'type') and 'cuda' in device.type:
                self.dtype = torch.float16
                self.device = device
                self.model = self.model.to(device)
                print(f"Model moved to GPU: {device}")
            else:
                self.dtype = torch.float32
                self.device = 'cpu'
                self.model = self.model.to('cpu')
                print(f"Model moved to CPU")
        elif self.is_multi_gpu:
            print("Multi-GPU model already optimally distributed")

    def prepare_single_input(self, image_list, question):
        """å‡†å¤‡å•ä¸ªæ ·æœ¬çš„è¾“å…¥ - ä¿®å¤ç‰ˆæœ¬"""
        try:
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": question},
                    ],
                }
            ]
            
            # æ·»åŠ å›¾åƒ
            for img in image_list:
                if img is not None:
                    messages[0]["content"].append({
                        "type": "image",
                        "image": get_image(np.array(img)),
                        "resized_height": 224,
                        "resized_width": 224
                    })
            
            # ç”Ÿæˆæ–‡æœ¬æ¨¡æ¿ - æ·»åŠ è‹±æ–‡æç¤º
                    text = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        # æ·»åŠ è‹±æ–‡å¼•å¯¼
        if not text.endswith("Assistant: "):
            text = text.rstrip() + " Please respond in English: "
            
            # å¤„ç†è§†è§‰ä¿¡æ¯ - æ·»åŠ é”™è¯¯å¤„ç†
            try:
                image_inputs, video_inputs = process_vision_info(messages)
                # ç¡®ä¿è¿”å›å€¼ä¸ä¸ºNone
                image_inputs = image_inputs if image_inputs is not None else []
                video_inputs = video_inputs if video_inputs is not None else []
            except Exception as e:
                print(f"âš ï¸ Warning: process_vision_info failed for sample: {e}")
                image_inputs, video_inputs = [], []
            
            return text, image_inputs, video_inputs, messages
            
        except Exception as e:
            print(f"âŒ Error preparing input: {e}")
            return None, [], [], None

    @torch.no_grad()  # æ·»åŠ æ— æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜
    def generate_image_batch_fixed(self, image_list_list, question_list):
        """ä¿®å¤çš„æ‰¹é‡æ¨ç†ç‰ˆæœ¬"""
        print(f"ğŸ”„ Fixed batch inference: {len(image_list_list)} samples")
        
        all_outputs = []
        
        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜æº¢å‡º
        for i in range(0, len(image_list_list), self.batch_size):
            batch_images = image_list_list[i:i + self.batch_size]
            batch_questions = question_list[i:i + self.batch_size]
            
            print(f"  Processing batch {i//self.batch_size + 1}/{math.ceil(len(image_list_list)/self.batch_size)}")
            
            try:
                # åˆ†åˆ«å‡†å¤‡æ¯ä¸ªæ ·æœ¬çš„è¾“å…¥
                batch_texts = []
                all_image_inputs = []
                all_video_inputs = []
                valid_samples = []
                
                for j, (img_list, question) in enumerate(zip(batch_images, batch_questions)):
                    text, image_inputs, video_inputs, messages = self.prepare_single_input(img_list, question)
                    if text is not None:
                        batch_texts.append(text)
                        all_image_inputs.extend(image_inputs)
                        all_video_inputs.extend(video_inputs)
                        valid_samples.append(j)
                
                if not batch_texts:
                    print("âŒ No valid samples in this batch, skipping...")
                    continue
                
                # å¤„ç†å™¨ç¼–ç  - æ·»åŠ é”™è¯¯å¤„ç†
                try:
                    inputs = self.processor(
                        text=batch_texts,
                        images=all_image_inputs if all_image_inputs else None,
                        videos=all_video_inputs if all_video_inputs else None,
                        padding=True,
                        return_tensors="pt",
                    )
                except Exception as e:
                    print(f"âŒ Processor encoding failed: {e}")
                    # é™çº§åˆ°å•æ ·æœ¬å¤„ç†
                    raise e
                
                # å°†è¾“å…¥ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
                if not self.is_multi_gpu:
                    inputs = inputs.to(self.device)
                
                # æ‰¹é‡æ¨ç† - ä¼˜åŒ–å‚æ•°
                generated_ids = self.model.generate(
                    **inputs, 
                    max_new_tokens=self.max_new_tokens,
                    do_sample=False,  # ç¡®å®šæ€§æ¨ç†ï¼Œæ›´å¿«
                    num_beams=1,      # ä¸ä½¿ç”¨beam searchï¼Œæ›´å¿«
                    use_cache=True,   # ä½¿ç”¨ç¼“å­˜åŠ é€Ÿ
                    pad_token_id=self.processor.tokenizer.eos_token_id,
                    eos_token_id=self.processor.tokenizer.eos_token_id,
                    repetition_penalty=1.2,  # å¢åŠ é‡å¤æƒ©ç½š
                    length_penalty=1.0,      # é•¿åº¦æƒ©ç½š
                    # ç§»é™¤temperatureé¿å…è­¦å‘Š
                )
                
                # è§£ç è¾“å‡º
                generated_ids_trimmed = [
                    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                ]
                
                batch_outputs = self.processor.batch_decode(
                    generated_ids_trimmed, 
                    skip_special_tokens=True, 
                    clean_up_tokenization_spaces=False
                )
                
                # ç¡®ä¿è¾“å‡ºæ•°é‡åŒ¹é…
                for j in range(len(batch_images)):
                    if j in valid_samples:
                        idx = valid_samples.index(j)
                        all_outputs.append(batch_outputs[idx] if idx < len(batch_outputs) else "Error: No output")
                    else:
                        all_outputs.append("Error: Invalid input")
                
                print(f"âœ… Batch {i//self.batch_size + 1} completed successfully")
                
                # æ¸…ç†GPUç¼“å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                print(f"âŒ Error in batch {i//self.batch_size + 1}: {e}")
                print(f"ğŸ”„ Falling back to single sample processing...")
                
                # é™çº§åˆ°å•ä¸ªæ ·æœ¬å¤„ç†
                for j, (img_list, question) in enumerate(zip(batch_images, batch_questions)):
                    try:
                        output = self.generate_image_single_fast(img_list, question)
                        all_outputs.append(output)
                    except Exception as e2:
                        print(f"âŒ Error in single sample {i+j}: {e2}")
                        all_outputs.append(f"Error: {str(e2)}")
        
        print(f"âœ… Fixed batch inference completed: {len(all_outputs)} outputs")
        return all_outputs

    @torch.no_grad()
    def generate_image_single_fast(self, image_list, question):
        """ä¼˜åŒ–çš„å•æ ·æœ¬æ¨ç†"""
        try:
            text, image_inputs, video_inputs, messages = self.prepare_single_input(image_list, question)
            
            if text is None:
                return "Error: Failed to prepare input"
            
            inputs = self.processor(
                text=[text],
                images=image_inputs if image_inputs else None,
                videos=video_inputs if video_inputs else None,
                padding=True,
                return_tensors="pt",
            )
            
            if not self.is_multi_gpu:
                inputs = inputs.to(self.device)
            
            # ä¼˜åŒ–çš„ç”Ÿæˆå‚æ•°
            generated_ids = self.model.generate(
                **inputs, 
                max_new_tokens=self.max_new_tokens,
                do_sample=False,
                num_beams=1,
                use_cache=True,
                pad_token_id=self.processor.tokenizer.eos_token_id,
                eos_token_id=self.processor.tokenizer.eos_token_id,
                repetition_penalty=1.1,  # æ·»åŠ é‡å¤æƒ©ç½š
            )
            
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output = self.processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )[0]
            
            return output
            
        except Exception as e:
            print(f"âŒ Single sample error: {e}")
            return f"Error: {str(e)}"

    def generate_image(self, image_list_list, question_list):
        """ä¸»è¦æ¥å£ï¼šä¼˜åŒ–ç‰ˆæœ¬"""
        print("ğŸš€ Qwen2.5-VL fixed and optimized inference begins")
        print(f"ğŸ“Š Input: {len(image_list_list)} samples")
        
        try:
            if len(image_list_list) > 1:
                # ä½¿ç”¨ä¿®å¤çš„æ‰¹é‡æ¨ç†
                outputs = self.generate_image_batch_fixed(image_list_list, question_list)
            else:
                # å•ä¸ªæ ·æœ¬
                outputs = [self.generate_image_single_fast(image_list_list[0], question_list[0])]
            
            print("âœ… Qwen2.5-VL fixed and optimized inference finished")
            return outputs
            
        except Exception as e:
            print(f"âŒ Critical error in generate_image: {e}")
            traceback.print_exc()
            # æœ€åçš„é™çº§æ–¹æ¡ˆ
            return [f"Critical Error: {str(e)}" for _ in range(len(image_list_list))] 